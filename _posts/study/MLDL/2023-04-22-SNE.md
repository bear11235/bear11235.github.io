---
title: "t-SNE 공부하기"
categories:
  - study
  - ML-DL
tags:
  - study
  - ML-DL
---

# SNE
Stochastic Neighbor Embedding의 약자. 여기서 임베딩의 의미는 고차원에서 저차원으로 매핑한다는 말이다.

고차원 공간에 있는 데이터 포인트들 사이의 거리, 혹은 이웃인 정도를 확률로서 표현하고, 그 확률을 유지하도록 저차원 공간으로 매핑하는 것. 데이터 자체는 왜곡이 될 수 있지만, 데이터 사이의 가까운 정도는 최대한 유지하고 싶다. 여기서 학습이라는 것은 고차원 공간에서 저차원 공간으로 매핑하는 방법을 학습하는 것.

고차원에서 이웃의 정도를 결정하는 방법? -> Squared Exponential 함수를 통해 거리를 측정하고 확률로 표현하기 위해서 normalize를 진행한다. n개의 데이터 포인트에 대해서 각각 n개의 데이터에 대한 이웃인 정도를 나타내기에 n by n 형태의 Neighbar Probability Matrix가 생성된다. 각 데이터 포인트 사이의 절대적 거리는 대칭적이지만, 기준이 되는 포인트에서 normalization constant는 다르기 때문에, probability matrix는 비대칭으로 표현될 수 있다.

## Perplexity
2 ** H(P_i)로 정의된다. 데이터끼리 뭉치고 흩어지는 정도를 정하는 파라미터. 대략 한 점에서 이웃이라고 생각하는 점의 개수라고 생각하면 될 듯.

## Cost Function
고차원과 저차원에서의 확률 행렬을 각각 P_ij, Q_ij라고 하면, 그 둘 사이의 차이값, 거리값, 괴리감은 확률 분포의 괴리를 표현하는 KL divergence로 표현한다. 

$$ J = KL(P||Q) $$

KL Divergence를 사용하면, gradient descent 방식에서 식이 쉽게 나온다. 

$$ dJ/dy = sth $$

gradient의 의미는? 이웃인 확률이 고차원 > 저차원 이라면 저차원 공간에서 서로 가까이 당겨야하고, 반대로 고차원 < 저차원 이라면 저차원 공간에서 서로 밀어야 한다. 또한 gradient의 크기가 저차원에서의 거리에 비례하기 때문에, 학습 과정에서 저차원 공간으로 매핑된 데이터들이 서로 뭉치게 된다.

실제 데이터들을 SNE를 통해 저차원으로 매핑하면, 데이터들끼리 좀 뭉치는 경향이 있다. 이를 보완하고자 t-SNE를 고안.

# t-SNE
t-SNE에서 t는 Student-t를 의미한다. 
기존 SNE보다 가까운 것은 더 가깝게, 먼 것은 더 멀게 표현하여 실제 데이터에 왜곡이 생길지라도 시각적으로 더 예쁘게 그리려고 함. SNE에서 이웃인 정도를 표현하기 위해 고차원과 저차원 공간에서 Gaussian 분포를 이용했다. 하지만 만약 저차원 공간에서 heavy-tailed distribution을 사용한다면? 고차원에서 가까운 점은 저차원에서 더 가까워지고, 고차원에서 먼 점은 저차원에서 더 멀어진다. 

t-SNE에서 cost function에 대한 gradient는 기존 SNE에서의 값에 (1+d)^-1 값이 붙는다. 이는 저차원 공간에서 두 점의 거리가 멀어지면 gradient 값이 작아지고, SNE와는 다르게 거리가 어느 이상 멀어지면 서로 밀고 당기는 힘의 크기가 작아진다. 그로 인해 가까운 데이터들은 서로 당기게 되고, 어느 거리가 넘어가면 더 이상 당기는 힘이 증가하지 않고 locally attract하게 된다.